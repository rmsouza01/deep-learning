# ENEL645 

This course is a hands-on course on Deep Learning (DL), which is a significant topic within machine learning.   This course will give an overview of the historical context that allowed DL to flourish. It will cover different types of neural networks, how to train, and deploy them in different problems, such as image classification, image segmentation, and signal denoising. The neural network types that will be covered are fully connected networks, convolutional neural networks, fully convolutional neural networks, auto-encoders, recurrent neural networks, and others. Special emphasis will be given to popular network architectures like U-nets, ResNets, Inception, and VGG. The course will cover how to fine-tune pre-trained models to achieve state-of-the-art results in relevant applications. This course will also give a brief introduction to generative models, self-supervised learning and an overview of current new trends in DL.

**The repository will be updated as the course progresses. We will be using Python 3 with TensorFlow.**

## Tutorials

### Week 01  
- **Tutorial 01**: [Introduction to Python](JNotebooks/tutorial01-python.ipynb)
- **Tutorial 02** [Introduction to NumPy](JNotebooks/tutorial02-numpy.ipynb)
- **Tutorial 03**: [Model Selection, Overfitting and Regularization](JNotebooks/tutorial03-overfitting_regularization.ipynb)

### Week 02  
- **Tutorial 04** [Fully Connected Neural Networks - 2D Synthetic Example ](JNotebooks/tutorial04_fully_connected_neural_network_2D_synthetic_example.ipynb)
- **Tutorial 04 (GOOGLE COLAB)**: [Fully Connected Neural Networks - 2D Synthetic Example ](JNotebooks/tutorial04_fully_connected_neural_network_2D_synthetic_example_colab.ipynb)
- **Tutorial 05**: [Different Approaches to Defining Neural Networks with Keras and TensorFlow](JNotebooks/tutorial05_different_approaches_to_define_neural_networks_keras.ipynb)

### Week 03  
- **Tutorial 06**: [TensorBoard](JNotebooks/tutorial06_tensorboard.ipynb)
- **Tutorial 07**:[Softmax, One Hot Encoding and Loss Functions](JNotebooks/tutorial07_softmax_one_hot_encoding_loss_functions.ipynb)
- **Tutorial 08**:[Step-by-step MNIST Digits Classification - Fully Connected Neural Networks](JNotebooks/tutorial08_step_by_step_MNIST_digits_classification.ipynb)

### Week 04
- **Tutorial 09** [Fully Connected Neural Networks Revisited - Destroying Local Correlations, what happens?](JNotebooks/tutorial09_fully_connected_neural_networks_revisited.ipynb)
- **Tutorial 10** [Step-by-step MNIST Digits Classification - Convolutional Neural Networks](JNotebooks/tutorial10_step_by_step_MNIST_digits_classification_cnn.ipynb)
- **Tutorial 11** [Transfer Learning](JNotebooks/tutorial11_transfer_learning_imagenet.ipynb)

### Week 05
- **Tutorial 12** [Auto-encoder MNIST](JNotebooks/tutorial12_auto_encoder_mnist.ipynb)


### Week 06
- Reading Week

### Week 07
- **Tutorial 13** [U-net for Image Restoration](JNotebooks/tutorial13_unet_jpeg_restoration.ipynb)
- **Tutotial 14** Using the cluster with Docker and/or virtualenv. The files are in the Cluster-example folder.

### Week 08
- **Tutorial 15** Using the TALC cluster. The files for this tutorial are inside TALC/Example01.

## Week 09
- **Tutorial 16** [Recurrent Neural Networks](JNotebooks/tutorial14_recurrent_neural_networks.ipynb)
- **Tutorial 17** [Generative Adversarial Neural Networks](JNotebooks/tutorial15_generative_adversarial_networks.ipynb)

## Week 10

## Week 11
- **Tutorial 17** [Self-Supervised Learning](JNotebooks/tutorial17_self_supervised_learning_CIFAR10_cnn.ipynb)

## Assignments

- [Assignment #02](Assignments/assignment02.ipynb)
- [Assignment #03](Assignments/assignment03.ipynb)
- [Assignment #04](Assignments/assignment04.ipynb)
